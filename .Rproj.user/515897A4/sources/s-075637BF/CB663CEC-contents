---
title: "week 5, report"
author: "Diana Hilleshein"
date: "2/15/2022"
header-includes:
  \usepackage{fvextra}
  \DefineVerbatimEnvironment{Highlighting}{Verbatim}{breaklines,commandchars=\\\{\}}
output: pdf_document
---
```{r}
setwd("C:/R scripts/DWA2022")
library(haven)

# import data
data <- read_sav("C:/R scripts/DWA2022/ESS9e03_1.sav")


save(data,file="data.csv")
head(data)
dim(data)#everything is right
```
From some previous courses recall topics: cross tabulation, Chi-Square test of independence and test for mean(s). If you find good sources, please add them to our database: https://moodle.helsinki.fi/mod/data/view.php?id=2537669.  

# Preperations  
As well as in a demo file, I will create the restricted data. I beleive it will will significantly increase speed of processing.  
Used for sum and avf avriables:
- *imsmetn* -Allow many/few immigrants of same race/ethnic group as majority (scale from 1 to 4, 1 = Allow many to come and live here, 4 = Allow none)  
- *imdfetn* -Allow many/few immigrants of different race/ethnic group from majority (scale from 1 to 4, 1 = Allow many to come and live here, 4 = Allow none)  
- *impcntr* -Allow many/few immigrants from poorer countries outside Europe (scale from 1 to 4, 1 = Allow many to come and live here, 4 = Allow none)  
*General information on a participants*  
1) *stflife* -How satisfied with life as a whole (from 0 to 10, 0 = extremely dissatisfied, 10 = extremely satisfied)  
2) *gndr*-Gender (1 = Male, 2 = Female)  
3) *agea* -Age of respondent, calculated (integer)  
4) **cntry** - Country of respondent (AT = Austria, BE = Belgium, BG = Bulgaria, CH = Switzerland, CY = Cyprus,  CZ = Czechia, DE = Germany, DK = Denmark, EE = Estonia, ES = Spain, FI = Finland, FR = France, GB = United Kingdom, HR = Croatia, HU = Hungary, IE = Ireland, IS = Iceland, IT = Italy, LT = Lithuania,  LV = Latvia, ME = Montenegro, NL = Netherlands, NO = Norway, PL = Poland, PT = Portugal, RS = Serbia,  SE = Sweden, SI = Slovenia, SK = Slovakia)  
```{r}
library(tidyverse)
library(dplyr)
library(ggplot2)

#create sum var
data$img_sum<-rowSums(select(data,imsmetn, imdfetn, impcntr), na.rm = TRUE)
head(data$img_sum)
#create mean var
data$img_avg<-rowMeans(select(data,imsmetn, imdfetn, impcntr), na.rm = TRUE)
head(data$img_avg)

data5 <- data %>% filter(cntry == "FI") %>% 
  select(., idno, cntry, agea, gndr, impcntr, img_sum, img_avg)
head(data5)
dim(data5)

summary(data5$agea)#average age is 51 years, minimum is 15 maximum is 90
summary(data5$impcntr) #average is 2.5
```


# Contingency table (aka cross tabulation)  
Choose two variables and create a contingency table (also known as a cross tabulation or crosstab). Discuss your findings.  

The interpretation for results: 

1) **gndr**-Gender (1 = Male, 2 = Female)  
2) **agea** -Age of respondent, calculated (integer)  
3) **cntry** - Country of respondent (AT = Austria, BE = Belgium, BG = Bulgaria, CH = Switzerland, CY = Cyprus,  CZ = Czechia, DE = Germany, DK = Denmark, EE = Estonia, ES = Spain, FI = Finland, FR = France, GB = United Kingdom, HR = Croatia, HU = Hungary, IE = Ireland, IS = Iceland, IT = Italy, LT = Lithuania,  LV = Latvia, ME = Montenegro, NL = Netherlands, NO = Norway, PL = Poland, PT = Portugal, RS = Serbia,  SE = Sweden, SI = Slovenia, SK = Slovakia)  
4) **impcntr** -Allow many/few immigrants from poorer countries outside Europe (scale from 1 to 4, 1 = Allow many to come and live here, 4 = Allow none)  


Contingency table
```{r}
t1 <- table(data5$gndr, data5$impcntr)
t1
```  
In the crosstab we can see the number of responses that fall under one of the categories. The number of observations in each cell in sufficient.  

Contingency table can be visualized using the function balloonplot() [in gplots package]. This function draws a graphical matrix where each cell contains a dot whose size reflects the relative magnitude of the corresponding component.  

*This website helped me a lot with the topic: <http://www.sthda.com/english/wiki/chi-square-test-of-independence-in-r>.  
```{r}
library(gplots)
library(ggplot2)
library(tidyverse)

balloonplot(t(t1), main ="gender and attitude to immigrants, t1", mtext("attitude to immigrants, 1 = allow many, 4 = allow none", side = 1), mtext ("gender, 1 = male, 2 = female", side = 2),
            label = T, show.margins = T)
```  
We can see a pattern here, women (2) tend to respond 1 or 2 while men (1) tend to respon 3 or 4. It is not reliable to count by number of response in each cell, but rather by persentage. I will make the same visualization but using persentages later.

It’s also possible to visualize a contingency table as a mosaic plot. This is done using the function mosaicplot() from the built-in R package garphics:  
```{r warning=FALSE}
mosaicplot(t1, shade = TRUE, las=2,
           main ="gender and attitude to immigrants, t1", xlab ="attitude to immigrants, 1 = allow many, 4 = allow none", ylab="gender, 1 = male, 2 = female",
            label = T, show.margins = T)
```  
- Blue color indicates that the observed value is higher than the expected value if the data were random  
- Red color specifies that the observed value is lower than the expected value if the data were random  
```{r}
#install.packages("janitor")
library(janitor)
t2<- data5 %>% tabyl(impcntr, gndr, show_na=F)
t2 #same as table()

t3 <- tabyl(data5$gndr, data5$impcntr, show_na=TRUE) #really nice package, good to see numbers and persetage at the same time. Also, NA is showed separately that is convinient.
t3
```
Number of responses are not always easy to observe especially when crosstabs are large. This is why is it beneficial to use prop.table(). I found some docuamntation on the function in using "?prop.table()".  

```{r}
prop.table(t1)
balloonplot(t(prop.table(t1, 1)), main ="gender and attitude to immigrants, t1", mtext("attitude to immigrants, 1 = allow many, 4 = allow none", side = 1), mtext ("gender, 1 = male, 2 = female", side = 2),
            label = T, show.margins = T)
balloonplot(t(prop.table(t1)), main ="gender and attitude to immigrants, t1", mtext("attitude to immigrants, 1 = allow many, 4 = allow none", side = 1), mtext ("gender, 1 = male, 2 = female", side = 2),
            label = T, show.margins = T)
```
The pattern didn`t change that means that the plot made those calculations before, when we just put numbers as an input.  

Here, what we used as an input data for the baloonplot().
```{r}
prop.table(t1) #devide by all cases
prop.table(t1, 1) #devide by all cases in a row
prop.table(t1, 2) #devide by all cases in a column
```

```{r}
# X^2-test
# H0: gender and happiness are statistically independent
# H1: gender and happiness are not statistically independent

chq1 <- chisq.test(data5$gndr, data5$impcntr)
chq2 <- data5 %>% tabyl(gndr, impcntr, show_na=F) %>% chisq.test()
chq1
chq2
```
Two different ways of coding are showing the same result. 
The row and the column variables are statistically significantly associated (p-value < 0).  

# observed vs. expected frequencies
```{r}
chq1$observed
chq2$expected
```
Expected and observed values differ a lot for column 1 (allow many).  

The most contributing cells to the total Chi-square score:
```{r}
# cell contributions:
round(chq1$residuals, 3)

library(corrplot)
corrplot(chq1$residuals, is.cor = FALSE, title = "The most contributing cells to the total Chi-square score")
```
For a given cell, the size of the circle is proportional to the amount of the cell contribution.  

- Positive residuals are in blue. Positive values in cells specify an attraction (positive association) between the corresponding row and column variables. For example, there is no association between male and allow many, but there is a strong asscoaition between male and don`t allow some.

- Negative residuals are in red. This implies a repulsion (negative association) between the corresponding row and column variables.

The most contributing cells to the total Chi-square score, contibution in percentage (%):  
```{r}
cell_contrib <- (chq1$observed - chq1$expected)^2/chq1$expected
round(cell_contrib, 3)


corrplot(cell_contrib, is.cor = FALSE, title = "The most contributing cells to the total Chi-square score,\n contibution in percentage (%): ")
```
The relative contribution of each cell to the total Chi-square score give some indication of the nature of the dependency between rows and columns of the contingency table.
The column males is strongly associacted with not allow immigrants at all, while the same association for women is slighly weaker. 

From the figure above, it can be seen that the most contributing cells to the Chi-square are male/allow many (6.367 %), female/allow many (5.924 %), male/do not allow some (2.101 %), females/do not tallow some (1.954 %).  

These cells contribute about 16.35% to the total Chi-square score and thus account for most of the difference between expected and observed values.


# Testing sum variable  
I will use sum variable from the previous exercise and gender variable. 
```{r}
hist(data5$img_sum)

# histogram with normal curve
x <- data5$img_sum; s <- sd(x, na.rm = TRUE); m <- mean(x, na.rm = TRUE)
hist(data5$img_sum, probability=TRUE)
curve(dnorm(x, mean=m, sd=s), add=TRUE)
library(ggpubr)
ggdensity(data$img_sum,  na.rm = T) 

#find skewness
#install.packages("moments")
library(moments)
#?skewness

print(skewness(data5$img_avg, na.rm = T))
```
The curve looks normal, but the distribution of responses does not look normal for me. I don`t know for with reason curve does not show skewness. However, skewness is -0.25, that indicantes that the data is fairly simmetrical according to <https://www.spcforexcel.com/knowledge/basic-statistics/are-skewness-and-kurtosis-useful-statistics#:~:text=So%2C%20when%20is%20the%20skewness,the%20data%20are%20highly%20skewed>.  

The density plots (curves) look very different and I cound`t understand what is the difference and wich one I need to use...

```{r}
# qqplot 
library(ggpubr)
ggqqplot(data5$img_sum)
```
The dots are following the line, with some shift at the upper end. The plot does not look good since there are some frequent categories of responses. The website that helped me to recall this matherial: <https://desktop.arcgis.com/en/arcmap/latest/extensions/geostatistical-analyst/normal-qq-plot-and-general-qq-plot.htm#:~:text=Points%20on%20the%20Normal%20QQ,deviate%20from%20the%20reference%20line.>. It seems like this plot shows that the variable is nornmally distributed.  

The Shapiro-Wilk’s test or Shapiro test is a normality test in frequentist statistics. The null hypothesis of Shapiro’s test is that the population is distributed normally. It is among the three tests for normality designed for detecting all kinds of departure from normality. If the value of p is equal to or less than 0.05, then the hypothesis of normality will be rejected by the Shapiro test. On failing, the test can state that the data will not fit the distribution normally with 95% confidence. 
```{r}
shapiro.test(data5$img_sum)
```
According to <https://www.geeksforgeeks.org/shapiro-wilk-test-in-r-programming/#:~:text=The%20Shapiro%2DWilk's%20test%20or,normality%20test%20in%20frequentist%20statistics.&text=If%20the%20value%20of%20p,distribution%20normally%20with%2095%25%20confidence.>, we can not assume normality. The p-value is smaller than 0.05. Hence, the distribution of the given data is significantly different from normal distribution.   
*This result made me confused :( . The  calculated skewness indicated that the distribution is normal, while the shapiro test rejected the H0: variable is normally distributed.*
```{r warning=FALSE}
ks.test(x = data5$img_sum, y = pnorm)
```
The test on the mean of the sample compares the hypothesis H0 with H1. The p-value is 2.2e-16. At risk 5%, we reject H0: <https://toltex.imag.fr/OneTestSPLS>. I can not conduct t-test since my distribution in not normal. Thereofre, I need to condct non parametric means test, Mann-Whitney test.

# means test
Before testing means, I need to create two subsets.
```{r}
# Test some fixed value
# H0: mu1 = mu2, H1: mu1 ne mu2
# males and females separately:
img_sum_males <- data  %>% filter(gndr == 1) %>% select(img_sum)
img_sum_females <- data  %>% filter(gndr == 2) %>% select(img_sum)


img_sum_males <- as.numeric(unlist(img_sum_males))
img_sum_females <- as.numeric(unlist(img_sum_females))


# test first the equality of variances
var.test(img_sum ~ gndr, data = data)
```
F value is quite close to 1, indicating that the variances are similar, p-value < 0.05. There is not significant difference between the variances of the two sets of data.  
Since the variation is not significantly different, I can assume that bot groups are not normally distributed. Therefore, I can not use t-test.  

Using the Mann-Whitney-Wilcoxon Test, we can decide whether the population distributions are identical without assuming them to follow the normal distribution.
```{r}
library(rstatix) #for second type of Mann-Whitney-Wilcoxon Test
#install.packages("rstatix")
#install.packages("coin")
library(coin)

data5$gndr <- as.data.frame(data5$gndr)
data5$gndr <- as.numeric(unlist(data5$gndr))
wilcox.test(img_sum_males, img_sum_females) #it was hard to interpret
data5 %>%wilcox_effsize(img_sum ~ gndr)#tried diffrent way
```
The Mann-Whitney U test results in a two-sided test p-value < 0.05. This indicates that we should reject the null hypothesis that distributions are equal and conclude that there is a significant difference between genders.  
<https://www.rdocumentation.org/packages/rstatix/versions/0.7.0/topics/wilcox_effsize>
helped to find an effect size.  
Here the results show a small effect size (0.077), therefore there is no statistical significance for the difference between two vectors. It can be vizualized as fillowing:

```{r}
data5$gndr <- as.factor(data5$gndr)
ggplot(data5, aes(img_sum, fill = gndr)) + 
  geom_bar(position = 'identity', alpha = 0.4)
```
To sum up, there is no big difference in attitues to immigrants between male and female respondents.
